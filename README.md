# **Multilingual MultiSpeaker Diarization in Conversational Settings – DISPLACE 2024 Challenge**

## **Project Overview**

This repository contains the implementation of a **speaker diarization system** developed for the **DISPLACE 2024 Challenge**, where I secured the **5th place**. The system is designed to perform diarization on a **multilingual, multi-speaker conversational dataset** using **spectral clustering**. 

**Key Features:**
- **Dataset:** DISPLACE 2024 challenge dataset with 32 training audios and 32 test audios, each lasting between **45-60 minutes**, covering **6-7 speakers per audio** and around **70-75 speakers in total.**
- **Model:** Experimented with various models, with the **UniSpeech Large model** delivering the best results.
- **Voice Activity Detection (VAD):** Utilized **pyannote-audio** for effective speech region detection.
- **Segmentation Strategy:** Employed a **1.5-second segment size** with an **overlapping window of 0.5 seconds.**
- **Evaluation Metric:** Diarization Error Rate (DER), achieving a DER of **34.67%** in the challenge.

---

## **Speaker Diarization Task**

Speaker diarization answers the question, "**Who spoke when?**" in an audio recording by:
1. Segmenting the audio into speech and non-speech regions.
2. Assigning speaker labels to the speech segments.
3. Generating an **RTTM (Rich Transcription Time Marked)** file with speaker timestamps.

---

## **DISPLACE Challenge Overview**

The **DISPLACE (DIarization of SPeaker and LAnguage in Conversational Environments) 2024 challenge** focused on **speaker diarization (SD), language diarization (LD), and automatic speech recognition (ASR)** in complex multilingual settings. The dataset consisted of:

- **158 hours of multilingual conversational speech,** including code-mixed and code-switched content.
- **45-60 minute sessions** with natural overlaps and challenging acoustic conditions.
- Data recorded in **Indian languages and Indian-accented English.**
- Data collected across four academic institutes to ensure diversity in recording environments.

The **speaker diarization task** aimed to identify **"who spoke when"** under noisy and overlapping speech conditions, requiring robust feature extraction and clustering strategies.

---

## **System Architecture**

The diarization pipeline consists of the following major components:

### **1. Voice Activity Detection (VAD)**
- Used the **pyannote-audio** library for detecting speech regions.
- Helps in reducing computational overhead by focusing on speech regions only.
- Output: Segments containing only speech.

### **2. Feature Extraction**
- Employed the **UniSpeech Large model**, which provided the best performance.
- Converts speech segments into speaker embeddings, capturing unique voice characteristics.

### **3. Clustering Approach**
- **Spectral Clustering** was used to group the extracted embeddings based on similarity.
- The number of speakers was estimated using clustering threshold tuning.

### **4. RTTM File Generation**
- Post-processing the cluster assignments to generate RTTM files, which include:
  - Speaker label
  - Start and end timestamps of speech segments.

---

## **Files in the Repository**

1. **`Displace.py`** – The main training script.
   - Implemented the Iterable Dataloader; it loads data in a streaming fashion, meaning they do not load the entire dataset into memory at once.
   - Handles data processing, feature extraction, and clustering.
   - This is particularly useful when dealing with large audio files (like 45-60 min durations in DISPLACE data), reducing memory footprint.
   - Calls the VAD module to segment speech.
   - Apply the clustering on the speaker embeddings generated by the model for each segments.
   - Saves diarization results in RTTM format.

3. **`model.py`** – Model definitions and experimentation.
   - Contains different speaker embedding extraction methods.
   - Includes the final configuration using the UniSpeech Large model.

4. **`utilities.py`** – Helper functions for diarization.
   - DER calculation.
   - RTTM conversion and handling (converting RTTM to speaker start-end times).
   - Segment creation functions using the specified segment size and overlap.

---

## **Evaluation Metric – Diarization Error Rate (DER)**

**Diarization Error Rate (DER)** is the primary metric used to evaluate the system performance. It measures the proportion of time the system incorrectly attributes speech to a speaker and consists of three components:

\[
DER = \frac{\text{False Alarm + Missed Speech + Speaker Confusion}}{\text{Total Speech Time}}
\]

- **False Alarm:** Non-speech labeled as speech.
- **Missed Speech:** Speech labeled as non-speech.
- **Speaker Confusion:** Speech assigned to the wrong speaker.

**Achieved DER in DISPLACE Challenge:** **34.67%**

## **Challenges Faced**

1. **Overlapping Speech Handling:**  
   - Addressing simultaneous speech from multiple speakers was challenging.
   - Improvements were made using the pyannote overlap detection module.

2. **Code-Mixed Speech:**  
   - The multilingual nature of the dataset introduced complexity in feature extraction.
   - Fine-tuning embeddings helped address this issue.

3. **Segment Length Optimization:**  
   - Balancing between accuracy and computational efficiency required careful tuning of segment size and overlap.

---

## **Future Improvements**

- Implementing **speaker embeddings fusion techniques** to further enhance clustering accuracy.
- Exploring **neural clustering approaches** to replace traditional spectral clustering.
- Integrating **adaptive window sizes** based on speech activity patterns.

---

## **Acknowledgments**

This work was developed as part of my participation in the **DISPLACE 2024 Challenge**, where I secured **5th place**. Thanks to the organizers for providing the challenging dataset and benchmarks.

---

## **References**

- DISPLACE Challenge 2024 Paper: [DISPLACE 2024 Challenge Paper](https://displace2024.github.io/)
- Pyannote-Audio VAD Documentation: [pyannote.audio](https://github.com/pyannote/pyannote-audio)
